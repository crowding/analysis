## Now here I start to look at the control experiments where I had variations.

## Note I am NOT running botostraps on this data. It appears that
## bootstraps fail for data generated by QUEST; this makes sense
## because while in method of constant stimulus you can think of each
## trial using the same stimulus as an independent draw from the
## distribution of responses for that stimulus, (with some fudging for
## being drawn with/without replacement) but when you run a QUEST the
## ensemble of stimuli is NOT independently drawn, and there are too
## few trials at most conditions outside the few that QUEST zeroes in
## on.

##common location for modeling functions
source("common.manipulations.r")
source("modeling.manipulations.R")

##we run one of these for each "variation pool"
my.args <- c("pools/SF.Rdata", "pools/SF.products")

pych.fits <- function(...) {
  my.args <- list(...)

  library(plyr)
  library(ggplot2)
  library(psyphy)
  library(boot)
  library(utils)
  library(arm)

  load(my.args[[1]])

  common.manipulations(environment())

  ## work out what we vere varying during these experiments...
  condition.columns <- what.varies(runs)

  trials <-
    within(trials,
           target.spacing <- 2 * pi * trial.motion.process.radius / trial.extra.nTargets)
  trials <- subset(trials, trial.version...function == "ConcentricTrial")
  trials <- transform(trials, log.target.spacing=log(target.spacing), target.spacing=NULL)

  ##strip away data/columns we aren't planning on using, for the time being.
  trials <- subset(trials, select=c(condition.columns,
                             'trial.motion.process.radius', 'result.success',
                             'log.target.spacing', 'motionCondition', 'subject',
                             'responseInWindow', 'responseTime', 'minResponseTime',
                             'maxResponseTime', 'correct', 'runs.i'))
  rm(triggers)
  rm(runs)
  rm(frame.skips)

  ##Calculate proportion correct among trials where the subject
  ##answered within the permitted time bounds.
  pCorrect <-
    ddply(subset(trials, responseInWindow & !is.na(responseTime)),
          c('subject', 'trial.motion.process.radius', 'motionCondition',
            'log.target.spacing', condition.columns),
          function(x) c(n = nrow(x), pCorrect=mean(x$correct)))

  ##Plot this raw data for the incongruent-motion trials.
  raw.data.plot <-
    ggplot(subset(pCorrect, motionCondition=='incongruent'),
           aes(log.target.spacing,
               pCorrect,
               colour = factor(trial.extra.wavelengthScalar))
           ) +
             geom_point() +
               opts(aspect.ratio=0.5) + 
                 facet_grid(trial.motion.process.radius ~ subject)
  print(raw.data.plot)

  ##psychometric function with guess rate
  fam <- binomial(link=logit.2asym(0.05, 0.05))

  ##model fitting function, produces a model with specified subset and formula.
  fit.model <- function(data, subset=NULL, formula=fmla) {
    ##Workaround for glm's nonstandard evaluation discussed here:
    ##https://stat.ethz.ch/pipermail/r-help/2008-February/154201.html
    eval(bquote((glm(formula, family=fam, data=data, subset=.(subset)))))
  }

  #which trials we are considering at the moment: the ones with the response in window.
  considered.trials <- with(trials,
                            (motionCondition == 'incongruent')
                            & responseInWindow
                            & !is.na(responseInWindow)
                            )

  #which conditions do we care about plotting, after fitting.
  conditions <-
    ddply(trials[considered.trials,],
          c('subject', 'trial.motion.process.radius', condition.columns),
          function(df) df[1,condition.columns,drop=FALSE])

  #some currying the library function to bind up our dataset
  fit.and.simulate.model <- function(fmla, data=trials[considered.trials,], iter=200, cond=conditions) {
    fit.and.simulate(fmla, data=data, iter=iter, cond = conditions)
  }

  #our modeling formulas TODO: make this generic for the thing being addressed.
  each.fmla <- correct ~ log.target.spacing*
    factor(trial.motion.process.radius)*
      factor(trial.extra.wavelengthScalar)*
        factor(subject)

  ##this computes a slope and offset by simulating the regression fit
  ##the slopes are inversely related to quantiles with the guessign
  ##rate factored out (logistic quantiles -- 9-91%) TODO: fit actual
  ##quantiles?
  each.fit <- fit.and.simulate.model(each.fmla)
  
  ##make a plot with intervals showing the "PSE" and the lower and upper quantiles.
  quantilePlot <-
    ggplot(each.fit$conditions, aes(log(trial.motion.process.radius), log.target.spacing, colour=factor(trial.extra.wavelengthScalar))) + geom_line() +
      geom_ribbon(aes(ymin=log.target.spacing-1/slope, ymax = log.target.spacing+1/slope, fill=factor(trial.extra.wavelengthScalar)), alpha=0.2) + 
      facet_grid(subject ~ ., scales="free_y") +
        opts(aspect.ratio=0.5, ymin=0, ymax=3)
  print(quantilePlot)

  ## that's ugly but promising. In the naice subjects I see a trend
  ## for the larger stimuli corresponding to a larger pritical
  ## spacing. There's some indication that the size of the transition
  ## changes with the stimulus size, esp. in subject DT, but it's inconsistent and could be
  ## colored by the QUEST procedure.  I'd better pool this data to
  ## have a better chance of discussing the size of the transition
  ## region. So what factors do I want?
  ##
  ## basic factors: model response rate as a function of:
  ##
  ## slope and offset changing with eccentricity, clearly.
  ##
  ## in terms of the GLM, "offset" means changing with and "slope" means multiplied by radius...
  ##
  ## let's try pooling one way then the other: fit a slope and offset
  ## that scales with radius, and is independent for each radius. Then
  ## fit a slope and offset that scales with stimulus size separately
  ## at each radius... plot the curve fits at each crossways to see
  ## what's going on.

  ##first, a model collapsing the eccentricities (per subject) into a
  ##scaling factor. separate offsets per subject and factor and
  ##separate slopes per sunject and factor.

  ##let's start with a simple dumb model: no dependence on the
  ##variated factor. but should be able to scale the size of hte
  ##transition region, if necessary
  simple.scaling.fmla <- correct ~
     factor(subject) +
       log.target.spacing:factor(subject) +
         log(trial.motion.process.radius):factor(subject) +
             log(trial.motion.process.radius):log.target.spacing:factor(subject)
  simple.scaling.fit <- fit.and.simulate.model(simple.scaling.fmla, iter=1)
  print(quantilePlot %+% simple.scaling.fit$conditions)

  simple.scaling.fmla <- correct ~
  factor(subject) + log.target.spacing+log(trial.motion.process.radius)
  
  ##OR MAYBE NOT: it scales the slope so hard
  ##that it reverses at one end for GB. (oops!) Ha, so actually what
  ##we need to fit is the "transition width" which is the INVERSE of the slope....
  simple.scaling.fmla <- update(
      simple.scaling.fmla, . ~ . - log(trial.motion.process.radius):log.target.spacing:factor(subject) )
  simple.scaling.fit <- fit.and.simulate.model(simple.scaling.fmla, iter=1)
  print(quantilePlot %+% simple.scaling.fit$conditions)

  ##what's unfortunate is that I don't yet see an easy way within the
  ##generalized linear regression to add to the WIDTH as opposed to
  ##the SLOPE. Nonlinearities again. Hmmmmm.

  ##Anyway...  question: aside from wanting to fit the width of the
  ##transition region as opposed to its slope, it is worthwhile having
  ##the interaction term,
  ##log(trial.motion.process.radius):log.target.spacing ??? One might
  ##return to the constant data for that. (TODO)

  ##This is interesting. until I solve this problem I will not be able
  ##to look too closely at the effect on "transition width" of feature
  ##sizes. Perhaps the answer is a nonlinear regression...

  ##Now back to accomplishing something rather than just finding a new
  ##obstacle.  For the moment I am willing to stipulate that the size
  ##of the transition region scales with eccentricity (using scaling
  ##features.)

  ##Let's consider a scaling fit for each feature size.
  feature.fmla <- update(simple.scaling.fmla, . ~ .*factor(trial.extra.wavelengthScalar))
  feature.fmla <- update(simple.scaling.fmla, . ~ . + .:factor(trial.extra.wavelengthScalar))
  ##why are these complaining about a rank deficient fit? Look at the matrix?

  feature3.fmla <- correct ~ factor(subject)*(log.target.spacing + log(trial.motion.process.radius))
  feature3.fit <- fit.and.simulate.model(feature3.fmla, iter=1)
  print(quantilePlot %+% feature3.fit$conditions)

  #eh, let's just mess with a lot of formulas
  trythis <- function(fmla, iter=1) {
    trythis.fit <- fit.and.simulate.model(fmla, iter=1)
    print(quantilePlot %+% trythis.fit$conditions)
    return(trythis.fit)
  }

  #scaling, simple linear law with the same slope (per subject)
  trythis(correct~factor(subject)*(log.target.spacing + log(trial.motion.process.radius))) 
  #separate offset per eccentricity but same transition width
  trythis(correct~factor(subject)*(log.target.spacing + factor(log(trial.motion.process.radius))))
  ##I suspect GB's data is crap in this experiment...

  #separate transition width per eccentricity but same offset?!
  trythis(correct~factor(subject)*(log.target.spacing + factor(log(trial.motion.process.radius))))
  
  ##here's a separate slope per eccentricity as well.
  trythis(correct~factor(subject)*(log.target.spacing * factor(trial.motion.process.radius)))
  ##even stronger suspicion that GB's data is crap... but DT and PBM seem OK.
  
  ##now how about adding an offset per each value of the feature size...?
  trythis(correct~factor(subject)*(log.target.spacing * factor(trial.motion.process.radius) + factor(trial.extra.wavelengthScalar)))

  #seems to be a consistent pattern: the larger features are associated with larger target spacings. But now notice what happens at the left of GB's plot: the "offset" scales with the "slope" that we already have, possibly misleading us. Now let's separate that out:
  trythis(correct~factor(subject)*(log.target.spacing * factor(trial.motion.process.radius)*factor(trial.extra.wavelengthScalar)))
  ##This is the same as "each.fit", effectively. I wonder why it is
  ##complaining about rank deficiency, but I'm not making predictions
  ##in any cases where I didn't test, so I press on. Not this is
  ##interesting, there seem to be systematic differences among GB and
  ##DT as far as slope vs. (and this splitting up slopes seems to
  ##rehabilitate GB's data somewhat?)

  ##Let's try simplifying the model again, Here, have a separate scaling law per feature size...?
  trythis(correct ~ factor(subject)*(log.target.spacing*trial.motion.process.radius*factor(trial.extra.wavelengthScalar)))
  ##Ah! this tells us that DT and maybe GB have this strange
  ##interaciton between feature size and slope -- and the interaction
  ##is in the same direction-- but this is less obvious for PBM.  and
  ##GB scales fine except at high spatial frequencies (which we
  ##expected, and knew this was one reason why he was difficult)

  ##okay, what about treating the different eccentricities as separate cases, but having a uniform effect of feature size? uniform in terms of MAGNITUDE OF SHIFT, that is.
  trythis(correct ~ factor(subject)*(factor(trial.motion.process.radius)*log.target.spacing*log(trial.extra.wavelengthScalar)))

  ##Notice that the re seems to be less effect of feature size in the
  ##center, as opposed to the periphery?!?! Can that be right?

  ##I'm still not sure that does the right thing from the standpoint of
  ##"feature size"... but it's interesting. These last two plots, or
  ##better plots made from the same fits, will be useful to look at all feature size variations.

  ##PLAN FOR TUESDAY: whatever I come up with, do it for all variation types. i.e. genericize this script. Also do a separate one for occlusions.
  ##Make up figure 1, so that I know what I'm doing there.

  ##PLAN FOR WEDNESDAY: synthesize these into figures and captions.
}

